---
title: "ğŸ§  çº¿æ€§ç¥ç»ç½‘ç»œ"
outline: deep

tags: "deepLearning"
updateTime: "2024-11-26 22:52"
---
![äº‘è¾¹å°å–éƒ¨](/public/window.jpg)
>æœ‰äººå“­ï¼Œæœ‰äººç¬‘ï¼Œæœ‰äººè¾“ï¼Œæœ‰äººè€ã€‚

# çº¿æ€§ç½‘ç»œ



## å‰è¨€


### æ•°æ®æ“ä½œå®ç°åŠé¢„å¤„ç†

***

```py
//å¯¼å…¥numpyåº“
import numpy as np
//åˆ›å»ºæ•°ç»„
np_array=np.array([1,3,4])

//ç¬¬äºŒç§æ–¹æ³•ï¼Œç”¨pytorchåº“
import torch
//åˆ›å»º4x5å…¨ä¸€çŸ©é˜µ
torch_tensor=torch.ones(4,5)
```

#### ä¸€ä¸ªæ¨¡å‹çš„é€šå¸¸æ­¥éª¤

- å¯¼å…¥ç›¸å…³åº“
- åˆå§‹åŒ–å˜é‡
- å¼•å…¥æ•°æ®é›†
- å®šä¹‰softmaxæˆ–è€…å…¶ä»–æ¦‚ç‡å¤„ç†å‡½æ•°
- å®šä¹‰ç½‘ç»œï¼Œè®¡ç®—y=xw+bå®ç°å‰å‘ä¼ æ’­
- å®šä¹‰æŸå¤±å‡½æ•°
- å®šä¹‰å‚æ•°æ›´æ–°çš„ä¼˜åŒ–å™¨
- è®­ç»ƒæ¨¡å‹
- é¢„æµ‹æ¨¡å‹

åœ¨ PyTorch ä¸­ï¼Œ`torch` å’Œ `tensor` éƒ½æœ‰è®¸å¤šå¸¸ç”¨çš„æ–¹æ³•ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸ç”¨çš„æ“ä½œå’Œæ–¹æ³•ï¼š

### 1. `torch` å¸¸ç”¨æ–¹æ³•ï¼š
`torch` æä¾›äº†åˆ›å»ºå¼ é‡å’Œå¤„ç†å¼ é‡çš„è®¸å¤šæ–¹æ³•ã€‚å¸¸è§çš„æœ‰ï¼š

- **åˆ›å»ºå¼ é‡ï¼š**
  
  ```python
  torch.tensor([1, 2, 3])  # ä» Python åˆ—è¡¨åˆ›å»ºå¼ é‡
  torch.ones(size)         # åˆ›å»ºå…¨ 1 çš„å¼ é‡
  torch.zeros(size)        # åˆ›å»ºå…¨ 0 çš„å¼ é‡
  torch.eye(n)             # åˆ›å»ºnxnå•ä½çŸ©é˜µ
  
  torch.arange(start, end, step)  # åˆ›å»ºèŒƒå›´å†…ç­‰å·®æ•°åˆ—å¼ é‡,è¯¥èŒƒå›´ä¸åŒ…å«endï¼Œå¦‚æœçœç•¥starté»˜è®¤ä»0å¼€å§‹åˆ°end-1
  ä¾‹å¦‚ï¼štorch.arange(1,3)ç»“æœï¼š[1,2]
  
  torch.matmul(å¼ é‡aï¼Œå¼ é‡b) #mat->matrix mul->multiplication çŸ©é˜µä¹˜æ³•
  torch.mul(a,b)#æ™®é€šçš„ä¹˜æ³•
  
  torch.normal(mean,std,size=(num,num))#ç”Ÿæˆéšæœºæ•°ï¼Œmeanæ˜¯å‡å€¼ï¼Œstdæ˜¯æ ‡å‡†å·®ï¼Œsizeæ˜¯å¼ é‡å½¢çŠ¶
  torch.linspace(start, end, steps)  # åˆ›å»ºèŒƒå›´å†…çº¿æ€§å‡åˆ†çš„å¼ é‡
  ```
  
- **éšæœºæ•°ç”Ÿæˆï¼š**
  ```python
  torch.rand(size)         # ç”Ÿæˆ [0, 1) åŒºé—´å†…å‡åŒ€åˆ†å¸ƒçš„éšæœºå¼ é‡
  torch.randn(size)        # ç”Ÿæˆæ ‡å‡†æ­£æ€åˆ†å¸ƒçš„éšæœºå¼ é‡
  torch.randint(low, high, size)  # ç”Ÿæˆæ•´æ•°éšæœºå¼ é‡
  ```

- **å¼ é‡åˆå§‹åŒ–ï¼š**
  ```python
  torch.empty(size)        # åˆ›å»ºä¸€ä¸ªæœªåˆå§‹åŒ–çš„å¼ é‡
  torch.full(size, value)  # åˆ›å»ºä¸€ä¸ªæŒ‡å®šå€¼çš„å¼ é‡
  ```

- **å¼ é‡æ“ä½œï¼š**
  ```python
  torch.cat(tensors, dim=0)# æ‹¼æ¥å¤šä¸ªå¼ é‡
  torch.dot(a,b) #è®¡ç®—ä¸¤ä¸ªä¸€ç»´å‘é‡çš„å†…ç§¯ 
  dim=0è¡Œæ”¹å˜ dim=1åˆ—æ•°æ”¹å˜
  ```

### 2. `tensor` å¯¹è±¡å¸¸ç”¨æ–¹æ³•ï¼š
`tensor` å¯¹è±¡æ˜¯ PyTorch çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œå¸¸ç”¨æ–¹æ³•æœ‰ï¼š

- **å½¢çŠ¶æ“ä½œï¼š**
  ```python
  tensor.reshape(num,num)#é‡å¡‘ä¸ºnum x numå¼ é‡ï¼Œè®¾ç½®ä¸º-1æ—¶ï¼Œè¡¨ç¤ºè‡ªåŠ¨è®¡ç®—ç»´åº¦
  #ä¾‹å¦‚ï¼šy.reshape(-1,3)ï¼Œè‹¥yæœ‰9ä¸ªå…ƒç´ ï¼Œåˆ™-1ä»£è¡¨9/3=3
  tensor.shape             # æŸ¥çœ‹å¼ é‡çš„å½¢çŠ¶
  tensor.size()            # è¿”å›å¼ é‡çš„å¤§å°
  tensor.view(shape)       # æ”¹å˜å¼ é‡çš„å½¢çŠ¶ï¼ˆç­‰æ•ˆ reshapeï¼‰
  ```
  
- **å¼ é‡æ•°å­¦æ“ä½œï¼š**
  
  ```python
  tensor.add(other)        # å¼ é‡ç›¸åŠ 
  tensor.sub(other)        # å¼ é‡ç›¸å‡
  tensor.mul(other)        # å…ƒç´ çº§ç›¸ä¹˜
  tensor.div(other)        # å…ƒç´ çº§ç›¸é™¤
  tensor.pow(exponent)     # æ¯ä¸ªå…ƒç´ æ±‚å¹‚
  tensor.sqrt()            # å¼ é‡å¼€å¹³æ–¹
  tensor.sum(dim)          # æŒ‡å®šç»´åº¦æ±‚å’Œ
  tensor.mean(dim)         # æŒ‡å®šç»´åº¦æ±‚å‡å€¼
  tensor.max(dim)          # è·å–æœ€å¤§å€¼
  tensor.min(dim)          # è·å–æœ€å°å€¼
  tensor.clamp(min, max)   # å°†å¼ é‡çš„å€¼é™åˆ¶åœ¨æŒ‡å®šèŒƒå›´å†…
  ```
  
- **ç´¢å¼•ä¸åˆ‡ç‰‡ï¼š**
  ```python
  tensor[index]            # é€šè¿‡ç´¢å¼•è®¿é—®å¼ é‡
  tensor[:, i]             # é€‰å–æŒ‡å®šç»´åº¦çš„åˆ‡ç‰‡
  tensor.index_select(dim, indices)  # æŒ‰ç´¢å¼•é€‰å–å¼ é‡çš„ç‰¹å®šå…ƒç´ 
  ```

- **ä¸ NumPy äº’æ“ä½œï¼š**
  ```python
  tensor.numpy()           # å°† PyTorch å¼ é‡è½¬ä¸º NumPy æ•°ç»„
  torch.from_numpy(numpy_array)  # å°† NumPy æ•°ç»„è½¬ä¸º PyTorch å¼ é‡
  ```

- **GPU æ“ä½œï¼š**
  ```python
  tensor.cuda()            # å°†å¼ é‡ç§»åŠ¨åˆ° GPU
  tensor.cpu()             # å°†å¼ é‡ç§»åŠ¨åˆ° CPU
  ```

- **æ¢¯åº¦ç›¸å…³ï¼š**
  ```python
  tensor.requires_grad_()  # è®¾ç½®å¼ é‡éœ€è¦è®¡ç®—æ¢¯åº¦
  tensor.grad              # è·å–å¼ é‡çš„æ¢¯åº¦
  tensor.backward()        # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
  ```

### ç¤ºä¾‹ä»£ç ï¼š
```python
import torch

# åˆ›å»ºä¸€ä¸ª 3x3 å¼ é‡
tensor = torch.rand(3, 3)

# å¼ é‡çš„æ•°å­¦æ“ä½œ
result = tensor.add(2)   # æ¯ä¸ªå…ƒç´ åŠ  2
result = tensor.sum(dim=1)  # å¯¹æ¯ä¸€è¡Œæ±‚å’Œ

# æ”¹å˜å½¢çŠ¶
reshaped = tensor.view(1, 9)

# GPU æ“ä½œ
if torch.cuda.is_available():
    tensor = tensor.cuda()

print(tensor)
```



## ç¥ç»ç½‘ç»œ



### çº¿æ€§ç¥ç»ç½‘ç»œ

***

#### çº¿æ€§å±‚å…¥é—¨(ç©·ä¸¾æ³•)

- ç»“æ„y=wxï¼Œåˆ©ç”¨ç©·ä¸¾æ³•æ‰¾åˆ°æœ€ä¼˜å‚æ•°w
- ç»˜åˆ¶w-losså›¾åƒ(åç»­éƒ½æ˜¯ç”¨epoch-loss)
- å¯»æ‰¾lossæœ€ä½ç‚¹å¯¹åº”çš„w

##### ä»£ç æ¼”ç¤º

```py
# çº¿æ€§ç½‘ç»œåŸºç¡€å…¥é—¨  åˆ©ç”¨ç©·ä¸¾æ³•å¯»æ‰¾æœ€é€‚è¶…å‚æ•°w  è¿™é‡Œå¿½ç•¥äº†bï¼Œy=wx
import numpy as np
# ç»˜å›¾å·¥å…·
import  matplotlib.pyplot as plt
from torch.nn.functional import mse_loss

# æ•°æ®é¢„å¤„ç†
x_data=[1.0,2.0,3.0]
y_data=[2.0,4.0,6.0]
# wåˆå§‹åŒ–ä¸º0
w=0
# å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®—ï¼ˆç©·ä¸¾ä¸éœ€è¦æ›´æ–°å‚æ•°å’Œæ¢¯åº¦ä¸‹é™ï¼‰
def forward(x):
    return x*w
# mseæŸå¤±ï¼Œå³å‡æ–¹å¹³å‡æŸå¤±
def Loss(x,y):
    y_hat=forward(x)
    return  (y_hat-y)*(y_hat-y)

# ç›´æ¥å¼€å§‹ç©·ä¸¾
# åˆå§‹åŒ–ä¸¤ä¸ªåˆ—è¡¨è£…wå’Œå¯¹åº”çš„æŸå¤±å€¼ï¼Œä¸ºåç»­ç»˜å›¾ä½¿ç”¨
w_list=[]
l_list=[]
# wä»0-4æŒ¨ä¸ªç©·ä¸¾
for w in np.arange(0.0,4.1,0.5):
    print('w=',w)
    l_sum = 0
    for (x,y) in zip(x_data,y_data):
        y_hat=forward(x)
        loss=Loss(x,y)

        # è®¡ç®—æ€»æŸå¤±
        l_sum+=loss
        print('\t',x,y,y_hat,loss)
    # æ·»åŠ åˆ°åˆ—è¡¨
    w_list.append(w)
    # å‡æ–¹æŸå¤±
    l_list.append(l_sum/3)
    print('mse=',l_sum/3)

# ç»˜åˆ¶å›¾åƒ
plt.plot(w_list,l_list)
# è®¾ç½®æ¨ªçºµåæ ‡
plt.ylabel('loss')
plt.xlabel('w')
# å±•ç¤º
plt.show()
```

#### çº¿æ€§å±‚å…¥é—¨ï¼ˆæ¢¯åº¦ä¸‹é™GDï¼‰

æ¢¯åº¦ä¸‹é™æ›´æ–°å…¬å¼ï¼šw=w - å­¦ä¹ ç‡*åå¯¼

##### ä»£ç æ¼”ç¤º

```py
"""
ä½¿ç”¨æ¢¯åº¦ä¸‹é™å¯»æ‰¾æœ€ä¼˜è¶…å‚æ•°w
"""
import numpy as np
import matplotlib.pyplot as plt

# æ•°æ®é¢„å¤„ç†
x_data=[1,2,3]
y_data=[2,4,6]

w=1
epoch_list=[]
loss_list=[]
def forward(x):
    return x*w
# å®šä¹‰æŸå¤±å’Œgd
def Loss(x,y):
    y_hat=forward(x)
    return (y_hat-y)*(y_hat-y)
# grad=2(x*w-y)*w
def grad_decent(x,y):
    return  2*(x*w-y)*w
for epoch in range(100):
    epoch_list.append(epoch)
    l_sum=0
    grad_sum=0
    for(x,y) in zip(x_data,y_data):
        # å•ä¸ªæŸå¤±
        loss=Loss(x,y)
        # å•ä¸ªæ¢¯åº¦
        grad=grad_decent(x,y)
        grad_sum+=grad
        l_sum+=loss
    w-=0.1*grad_sum/3
    loss_list.append(l_sum / 3)
    print('epoch=',epoch,'w=',w,'loss=',l_sum/3)
print('x=4,predicted=',forward(4))
# ç»˜å›¾
plt.plot(epoch_list,loss_list)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()
```



#### çº¿æ€§å±‚ï¼ˆéšæœºæ¢¯åº¦ä¸‹é™SGDï¼‰

- ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ï¼Œä¸åœ¨æ˜¯ä½¿ç”¨æ‰€æœ‰æŸå¤±çš„å’Œl_sumçš„å‡å€¼ä½œä¸ºæŸå¤±ï¼Œè€Œæ˜¯å°†æ¯ä¸€ä¸ªlossä½œä¸ºæŸå¤±å¹¶ä¸”æ±‚å¯¼
- SGDéšæœºæ¢¯åº¦ä¸‹é™ ä¸æ¢¯åº¦ä¸‹é™GDç›¸æ¯”
- ä¼˜ç‚¹ï¼šç»“æœæ›´ç²¾ç¡®ï¼Œæ€§èƒ½æ›´å¥½
- ç¼ºç‚¹ï¼šæ‰€éœ€æ—¶é—´æ›´é•¿
  - æ—¢è¦æ€§èƒ½åˆè¦æ—¶é—´æ•ˆç‡ï¼ŒæŠ˜ä¸­ï¼Œä½¿ç”¨miniBatchï¼Œå°†æ¯ä¸ªepochåˆ†ä¸ºå°æ‰¹é‡ä½¿ç”¨sgd(åç»­ä½¿ç”¨)

##### ä»£ç æ¼”ç¤º

```py
"""
ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ï¼Œä¸åœ¨æ˜¯ä½¿ç”¨æ‰€æœ‰æŸå¤±çš„å’Œl_sumçš„å‡å€¼ä½œä¸ºæŸå¤±ï¼Œè€Œæ˜¯å°†æ¯ä¸€ä¸ªlossä½œä¸ºæŸå¤±å¹¶ä¸”æ±‚å¯¼
"""
# SGDéšæœºæ¢¯åº¦ä¸‹é™ ä¸æ¢¯åº¦ä¸‹é™GDç›¸æ¯”
# ä¼˜ç‚¹ï¼šç»“æœæ›´ç²¾ç¡®ï¼Œæ€§èƒ½æ›´å¥½
# ç¼ºç‚¹ï¼šæ‰€éœ€æ—¶é—´æ›´é•¿
# æ—¢è¦æ€§èƒ½åˆè¦æ—¶é—´æ•ˆç‡ï¼ŒæŠ˜ä¸­ï¼Œä½¿ç”¨miniBatchï¼Œå°†æ¯ä¸ªepochåˆ†ä¸ºå°æ‰¹é‡ä½¿ç”¨sgd

import matplotlib.pyplot as plt

# æ•°æ®é¢„å¤„ç†
x_data=[1,2,3]
y_data=[2,4,6]

w=1

def forward(x):
    return x*w
# å®šä¹‰æŸå¤±å’Œgd
def Loss(x,y):
    y_hat=forward(x)
    return (y_hat-y)*(y_hat-y)
# grad=2(x*w-y)*w
def grad_decent(x,y):
    return  2*(x*w-y)*w
for epoch in range(100):

    grad_sum=0
    for(x,y) in zip(x_data,y_data):
        # å•ä¸ªæŸå¤±
        loss=Loss(x,y)
        # å•ä¸ªæ¢¯åº¦
        grad=grad_decent(x,y)
        # ä¸å†æ ¹æ®æŸå¤±çš„å’Œä½œä¸ºæ¢¯åº¦æ›´æ–°ï¼Œè€Œæ˜¯æ¯ä¸€ä¸ªéƒ½ä¼šæ›´æ–°
        w -= 0.1 * grad
        print('epoch=', epoch, 'w=', w, 'loss=', loss)


print('x=4,predicted=',forward(4))
# ç»˜å›¾
# plt.plot(epoch_list,loss_list)
# plt.xlabel('epoch')
# plt.ylabel('loss')
# plt.show()
```





#### çº¿æ€§å±‚ï¼ˆpytorchå®ç°ï¼‰

***

ä¹‹å‰æˆ‘ä»¬çš„æ¢¯åº¦éƒ½æ˜¯æ ¹æ®å…·ä½“çš„æ¨¡å‹å¾—å‡ºæ¥çš„è®¡ç®—å…¬å¼ï¼Œä½†æ˜¯åœ¨ç°å®çš„æ¨¡å‹é‡Œé¢ï¼Œä¸”ä¸è¯´èƒ½ä¸èƒ½å¾—å‡ºï¼Œå…¶å¤æ‚ç¨‹åº¦éš¾ä»¥æƒ³è±¡
å› æ­¤ç”¨tensorå¼ é‡çš„å½¢å¼ï¼Œå°†è®¡ç®—å›¾ä¿å­˜ä¸‹æ¥ï¼Œæ¯æ¬¡åªéœ€è¦backwardå°±å¯ä»¥å°†æ¯ä¸€ä¸ªèŠ‚ç‚¹çš„æ¢¯åº¦å€¼è®¡ç®—å‡ºæ¥å¹¶ä¸”ä¿å­˜

##### ä»£ç æ¼”ç¤º

```py
# å¼•å…¥pytorchå¼ é‡ï¼Œå®ç°åå‘ä¼ æ’­
"""
ä¹‹å‰æˆ‘ä»¬çš„æ¢¯åº¦éƒ½æ˜¯æ ¹æ®å…·ä½“çš„æ¨¡å‹å¾—å‡ºæ¥çš„è®¡ç®—å…¬å¼ï¼Œä½†æ˜¯åœ¨ç°å®çš„æ¨¡å‹é‡Œé¢ï¼Œä¸”ä¸è¯´èƒ½ä¸èƒ½å¾—å‡ºï¼Œå…¶å¤æ‚ç¨‹åº¦éš¾ä»¥æƒ³è±¡
å› æ­¤ç”¨tensorå¼ é‡çš„å½¢å¼ï¼Œå°†è®¡ç®—å›¾ä¿å­˜ä¸‹æ¥ï¼Œæ¯æ¬¡åªéœ€è¦backwardå°±å¯ä»¥å°†æ¯ä¸€ä¸ªèŠ‚ç‚¹çš„æ¢¯åº¦å€¼è®¡ç®—å‡ºæ¥å¹¶ä¸”ä¿å­˜
"""
import torch

# æ•°æ®é¢„å¤„ç† è™½ç„¶è¿™é‡Œå®šä¹‰çš„æ—¶å€™æ˜¯åˆ—è¡¨å½¢å¼ï¼Œä½†æ˜¯è·Ÿwç›¸ä¹˜åä¼šè‡ªåŠ¨è½¬ä¸ºå¼ é‡ï¼Œæ‰€ä»¥ä¹Ÿå¯ä»¥å®šä¹‰ä¸ºå¼ é‡ ä»¥ä¸‹ä¸¤ç§ç­‰æ•ˆ
x_data = torch.tensor([[1.0], [2.0], [3.0]])
y_data = torch.tensor([[2.0], [4.0], [6.0]])

# x_data=[1,2,3]
# y_data=[2,4,6]
# æ³¨æ„è¿™é‡Œçš„trueé¦–å­—æ¯è¦å¤§å†™ï¼Œä¸ç„¶è¯†åˆ«ä¸äº†
# require_gradè¡¨ç¤ºæ˜¯å¦éœ€è¦æ±‚æ¢¯åº¦
w=torch.Tensor([1.0])
w.requires_grad=True

def forward(x):
    return x*w
def Loss(x,y):
    y_hat=forward(x)
    return (y_hat-y)**2

for epoch in range(100):
    for (x,y) in zip(x_data,y_data):
        # å‰å‘ä¼ æ’­å¾—åˆ°loss
        loss=Loss(x,y)
        # åå‘ä¼ æ’­å°†è®¡ç®—å›¾ä¸­æ¯ä¸ªèŠ‚ç‚¹çš„æ¢¯åº¦éƒ½ä¿å­˜ä¸‹æ¥
        loss.backward()
        # sgdä¼˜åŒ–
        """
        ä¸èƒ½ç›´æ¥w-=0.01*w.grad
        å› ä¸ºåªè¦wä¸ºå¼ é‡ï¼Œé‚£ä¹ˆæ ¹æ®wå¼•å‡ºæ¥çš„è®¡ç®—å›¾ä¸­æ‰€æœ‰å˜é‡éƒ½æ˜¯å¼ é‡ï¼Œå¼ é‡ä¸­ä¸ä»…åŒ…å«äº†dataå€¼ï¼Œè¿˜ä¿å­˜äº†ç›¸åº”çš„æ¢¯åº¦å€¼
        æ‰€ä»¥è¦ç”¨.dataæ¥æ‹¿åˆ°å€¼çš„å¯¹è±¡ï¼ŒåŒæ—¶é‡Œé¢çš„gradä¹Ÿæ˜¯tensorï¼Œæ‰€ä»¥ä¹Ÿè¦ç”¨dataï¼Œè¿™æ ·æ‰ä¸ä¼šå½±å“åˆ°è®¡ç®—å›¾
        è‹¥æ˜¯ä¸€ä¸ªæ ‡é‡å½¢å¼çš„tensorï¼Œé‚£ä¹ˆ.itemå°±å¯ä»¥ç›´æ¥æ‹¿åˆ°æ ‡é‡å€¼è€Œétensorå¯¹è±¡
        """
        w.data-=0.1*w.grad.data
        # å°†wçš„æ¢¯åº¦å€¼æ¸…é›¶ï¼Œä¸ç„¶æ¯æ¬¡åå‘ä¼ æ’­éƒ½ä¼šå åŠ 
        w.grad.data.zero_()
# åŒæ ·è¿™é‡Œä¸èƒ½ä½¿ç”¨forwardï¼ˆ4ï¼‰æ¥å¾—åˆ°è¾“å‡ºï¼Œä¸ç„¶ä¼šè¾“å‡ºä¸€ä¸ªtensorå¯¹è±¡
# å› ä¸ºwæ˜¯tensorï¼Œx*wä¼šè‡ªåŠ¨å°†xä¹Ÿå˜æˆtensor
print("x=4, y=",forward(4).item())



```

##### ä»£ç æ¼”ç¤º2

```py
"""
ä½¿ç”¨pytorchçš„nn.moduleæ„é€ æ¨¡å‹å®ç°çº¿æ€§å›å½’ï¼ˆnn=net nuralç¥ç»ç½‘ç»œï¼Œåªè¦æ˜¯ç¥ç»ç½‘ç»œçš„æ¨¡å‹å®ç°éƒ½éœ€è¦ç»§æ‰¿è‡ªnnï¼‰

"""
import torch
import torch.nn as nn



# æ•°æ®é¢„å¤„ç†
x_data = torch.tensor([[1.0], [2.0], [3.0]])
y_data = torch.tensor([[2.0], [4.0], [6.0]])

# å®šä¹‰ç½‘ç»œæ¨¡å‹
class Linear(nn.Module):
    # __init__æ˜¯pyçš„æ„é€ å™¨ï¼Œå¿…é¡»è¦æœ‰
    def __init__(self):
        # è¿™å°±æ˜¯è°ƒç”¨çˆ¶ç±»çš„æ„é€ å™¨ï¼Œæ‰€æœ‰çš„æ¨¡å‹ç…§ç€å†™å°±è¡Œäº†ï¼Œä¸ç”¨æ·±ç©¶
        super(Linear,self).__init__()
        # torch.nn.linearæœ‰ä¸‰ä¸ªå‚æ•°ï¼Œåˆ†åˆ«æ˜¯è¾“å…¥ç»´åº¦ï¼Œè¾“å‡ºç»´åº¦ï¼Œæ˜¯å¦æœ‰åç½®é‡b(é»˜è®¤ä¸ºtrue)ï¼Œè¿”å›çš„æ˜¯ä¸€ä¸ªå¯¹è±¡
        self.linear=torch.nn.Linear(1,1,bias=True)
    # forwardæ˜¯moduleå¼ºåˆ¶å­ç±»å®ç°çš„ï¼Œè€Œä¸”å¿…é¡»å°±å«è¿™åå­—ï¼Œè¿”å›çš„æ˜¯è¾“å‡ºç»“æœ
    def forward(self,x):
        # linearå®ç°y=wx+bå¹¶è¿”å›ç»“æœ
        y_pred=self.linear(x)
        return y_pred

# å®ä¾‹åŒ–model
model=Linear()
# å®šä¹‰æŸå¤±å’Œä¼˜åŒ–å™¨

# åªæœ‰ä¸€ä¸ªå‚æ•°ï¼Œå†³å®šæ±‚å’Œåè¦ä¸è¦æ±‚å‡å€¼ï¼ˆå¹¶ä¸å½±å“ï¼‰
criterion = torch.nn.MSELoss(reduction='mean')
# model.parameters()èƒ½è·å¾—æ‰€æœ‰æœ‰æ¢¯åº¦çš„å‚æ•°ï¼Œå°†è¿™äº›å‚æ•°éƒ½æ›´æ–°æƒé‡ï¼Œlræ˜¯å­¦ä¹ ç‡ï¼ˆæ­¥é•¿ï¼‰ï¼Œè¿˜èƒ½è®¾ç½®æƒé‡è¡°é€€ç­‰å‚æ•°
optimizer=torch.optim.SGD(model.parameters(),lr=0.01)
# è®­ç»ƒ
for epoch in range(10000):
    # è·å¾—è¾“å‡º
    y_pred=model(x_data)
    loss=criterion(y_pred,y_data)

    # æ¢¯åº¦æ¸…é›¶ååå‘ä¼ æ’­æ›´æ–°å‚æ•°
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    # æ¯100ä¸ªæ‰“å°ä¸€æ¬¡è®­ç»ƒä¿¡æ¯
    if (epoch + 1) % 100 == 0:
        print(f"Epoch [{epoch + 1}/100], Loss: {loss.item():.4f}")
# æ‰“å°æ›´æ–°ç»“æœ
print(f"w={model.linear.weight.item()},b={model.linear.bias.item()}")

# æµ‹è¯•
x_test = torch.tensor([[4.0]])
print(f"y_test={model(x_test).item()}")
```



#### çº¿æ€§å±‚ï¼ˆäºŒåˆ†ç±»ï¼‰

##### ä»£ç å®ç°

```py
"""
ç”±çº¿æ€§å›å½’->äºŒåˆ†ç±»ï¼ˆé€»è¾‘æ–¯è’‚å›å½’ï¼‰
äºŒåˆ†ç±»é‡è¦çš„æ˜¯æ±‚yä¸y_hatåˆ†å¸ƒçš„å·®å¼‚åŒæ—¶è¾“å‡ºå€¼åœ¨0-1åŒºé—´
å› ä¸ºä¸æ˜¯å€¼å¾—å·®å¼‚ï¼Œè€Œæ˜¯åˆ†å¸ƒçš„å·®å¼‚ï¼Œæ‰€ä»¥ç”¨åˆ°äº¤å‰ç†µæŸå¤±ï¼ˆäºŒç»´ï¼‰ã€‚

ä¸çº¿æ€§å›å½’ä¸€å…±æœ‰ä¸‰å¤„å·®å¼‚ï¼Œå…¶ä»–å®Œå…¨ç›¸åŒ
éœ€è¦å¼•å…¥functionalï¼Œç”¨sigmoidå°†è¾“å‡ºå€¼å›ºå®šåœ¨0-1
æŸå¤±ä½¿ç”¨äº¤å‰ç†µbceè€Œémse
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import  matplotlib.pyplot as plt

# æ•°æ®é¢„å¤„ç†
x_data = torch.tensor([[1.0], [2.0], [3.0]])
# ä¸æ˜¯é¢„æµ‹ï¼Œè€Œæ˜¯åˆ†ç±»
y_data = torch.tensor([[0], [0], [1.0]])

# å®šä¹‰ç½‘ç»œæ¨¡å‹
class Linear(nn.Module):
    # __init__æ˜¯pyçš„æ„é€ å™¨ï¼Œå¿…é¡»è¦æœ‰
    def __init__(self):
        # è¿™å°±æ˜¯è°ƒç”¨çˆ¶ç±»çš„æ„é€ å™¨ï¼Œæ‰€æœ‰çš„æ¨¡å‹ç…§ç€å†™å°±è¡Œäº†ï¼Œä¸ç”¨æ·±ç©¶
        super(Linear,self).__init__()
        # torch.nn.linearæœ‰ä¸‰ä¸ªå‚æ•°ï¼Œåˆ†åˆ«æ˜¯è¾“å…¥ç»´åº¦ï¼Œè¾“å‡ºç»´åº¦ï¼Œæ˜¯å¦æœ‰åç½®é‡b(é»˜è®¤ä¸ºtrue)ï¼Œè¿”å›çš„æ˜¯ä¸€ä¸ªå¯¹è±¡
        self.linear=torch.nn.Linear(1,1,bias=True)
    # forwardæ˜¯moduleå¼ºåˆ¶å­ç±»å®ç°çš„ï¼Œè€Œä¸”å¿…é¡»å°±å«è¿™åå­—ï¼Œè¿”å›çš„æ˜¯è¾“å‡ºç»“æœ
    def forward(self,x):
        # linearå®ç°y=wx+bå¹¶è¿”å›ç»“æœ
        y_pred=F.sigmoid(self.linear(x)) #ç¬¬äºŒå¤„å·®å¼‚
        return y_pred

# å®ä¾‹åŒ–model
model=Linear()
# å®šä¹‰æŸå¤±å’Œä¼˜åŒ–å™¨

# åªæœ‰ä¸€ä¸ªå‚æ•°ï¼Œå†³å®šæ±‚å’Œåè¦ä¸è¦æ±‚å‡å€¼ï¼ˆå¹¶ä¸å½±å“ï¼‰
criterion = torch.nn.BCELoss(reduction='mean')#ç¬¬ä¸‰å¤„å·®å¼‚
# model.parameters()èƒ½è·å¾—æ‰€æœ‰æœ‰æ¢¯åº¦çš„å‚æ•°ï¼Œå°†è¿™äº›å‚æ•°éƒ½æ›´æ–°æƒé‡ï¼Œlræ˜¯å­¦ä¹ ç‡ï¼ˆæ­¥é•¿ï¼‰ï¼Œè¿˜èƒ½è®¾ç½®æƒé‡è¡°é€€ç­‰å‚æ•°
optimizer=torch.optim.SGD(model.parameters(),lr=0.01)
# è®­ç»ƒ
for epoch in range(10000):
    # è·å¾—è¾“å‡º
    y_pred=model(x_data)
    loss=criterion(y_pred,y_data)

    # æ¢¯åº¦æ¸…é›¶ååå‘ä¼ æ’­æ›´æ–°å‚æ•°
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    # æ¯100ä¸ªæ‰“å°ä¸€æ¬¡è®­ç»ƒä¿¡æ¯
    if (epoch + 1) % 100 == 0:
        print(f"Epoch [{epoch + 1}/100], Loss: {loss.item():.4f}")
# æ‰“å°æ›´æ–°ç»“æœ
print(f"w={model.linear.weight.item()},b={model.linear.bias.item()}")

# æµ‹è¯•
x_test = torch.tensor([[4.0]])
print(f"y_test={model(x_test).item()}")


# ç”¨pltå±•ç¤ºå›¾åƒ æ›²çº¿éå¸¸è¿‘ä¼¼sigmoid
x=np.linspace(0,10,200)
x_t=torch.Tensor(x).view(200,1)
y_t=model(x_t)
y=y_t.data.numpy()
plt.plot(x,y)
plt.plot([0,10],[0.5,0.5],c='r')
plt.xlabel('Hours')
plt.ylabel('probability of pass')
plt.grid()
plt.show()
```



#### çº¿æ€§å±‚ï¼ˆå¤šåˆ†ç±»é—®é¢˜ï¼‰

***

##### ä»£ç å®ç°

```py
import torch
# å¼•å…¥æ•°æ®é›†å’Œé¢„å¤„ç†
from torchvision import datasets, transforms
# å¼•å…¥ DataLoader ç”¨äºåŠ è½½æ•°æ®é›†
from torch.utils.data import DataLoader
# å¼•å…¥ä¼˜åŒ–å™¨
import torch.optim as optim
# å¼•å…¥æ¿€æ´»å‡½æ•°å’Œäº¤å‰ç†µæŸå¤±å‡½æ•°
import torch.nn.functional as F
# ä¸€æ¬¡è®­ç»ƒå¤„ç†64ä¸ªæ ·æœ¬
batch_size = 64

# æ•°æ®é¢„å¤„ç†
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
# åŠ è½½æ‰‹å†™æ•°å­—æ•°æ®é›†
train_dataset = datasets.MNIST(root='../dataset/minist/', train=True, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
# åŠ è½½æµ‹è¯•æ•°æ®é›†
test_dataset = datasets.MNIST(root='../dataset/minist/', train=False, download=True, transform=transform)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# å®šä¹‰ç¥ç»ç½‘ç»œ
class Net(torch.nn.Module):
    def __init__(self):
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–å‡½æ•°
        super(Net, self).__init__()
        # å®šä¹‰ç½‘ç»œç»“æ„ 
        self.fc1 = torch.nn.Linear(784, 512)  # è¾“å…¥å±‚åˆ°ç¬¬ä¸€éšè—å±‚
        self.fc2 = torch.nn.Linear(512, 256)  # ç¬¬ä¸€éšè—å±‚åˆ°ç¬¬äºŒéšè—å±‚
        self.fc3 = torch.nn.Linear(256, 128)  # ç¬¬äºŒéšè—å±‚åˆ°ç¬¬ä¸‰éšè—å±‚
        self.fc4 = torch.nn.Linear(128, 64)   # ç¬¬ä¸‰éšè—å±‚åˆ°ç¬¬å››éšè—å±‚
        self.fc5 = torch.nn.Linear(64, 10)    # ç¬¬å››éšè—å±‚åˆ°è¾“å‡ºå±‚

    def forward(self, x):
        x = x.view(-1, 784)  # å°†è¾“å…¥å±•å¹³æˆ (batch_size, 784)
        x = F.relu(self.fc1(x))  # ReLU æ¿€æ´»å‡½æ•°
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = F.relu(self.fc4(x))
        x = self.fc5(x)  # è¾“å‡ºå±‚ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°ï¼ˆäº¤å‰ç†µæŸå¤±å‡½æ•°å†…éƒ¨ä¼šåº”ç”¨ softmaxï¼‰
        return x

# å®ä¾‹åŒ–æ¨¡å‹ã€å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
model = Net()
criterion = torch.nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)

# è®­ç»ƒè¿‡ç¨‹
def train(epoch):
    running_loss = 0.0
    for batch_idx, (inputs, targets) in enumerate(train_loader):
        optimizer.zero_grad()  # æ¢¯åº¦æ¸…é›¶
        outputs = model(inputs)  # å‰å‘ä¼ æ’­
        loss = criterion(outputs, targets)  # è®¡ç®—æŸå¤±
        loss.backward()  # åå‘ä¼ æ’­
        optimizer.step()  # æ›´æ–°å‚æ•°

        running_loss += loss.item()
        if batch_idx % 300 == 299:    # æ¯300ä¸ªmini-batchæ‰“å°ä¸€æ¬¡æŸå¤±
            print(f'[{epoch + 1}, {batch_idx + 1}] loss: {running_loss / 300:.3f}')
            running_loss = 0.0

# æµ‹è¯•è¿‡ç¨‹
def test():
    correct = 0
    total = 0
    with torch.no_grad():  # æµ‹è¯•æ—¶ä¸éœ€è¦è®¡ç®—æ¢¯åº¦
        for images, labels in test_loader:
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)  # è·å–æœ€å¤§æ¦‚ç‡çš„ç±»åˆ«
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f} %')

# è®­ç»ƒå’Œæµ‹è¯•æ¨¡å‹
if __name__ == '__main__':
    for epoch in range(10):
        train(epoch)
        test()

```

#####  å½“å‰ç†è§£

***

- å…ˆè°ƒç”¨ç›¸å…³åŒ…ï¼Œä»¥åŠè¿›è¡Œæ•°æ®é›†çš„ä¸€äº›é¢„å¤„ç†
- ç»§æ‰¿torch.nn.moduleæ­å»ºæ¨¡å‹
- å®ä¾‹åŒ–ï¼Œå®šä¹‰å¥½æŸå¤±å’Œä¼˜åŒ–å‡½æ•°
- è¿›è¡Œè®­ç»ƒï¼šå‰å‘ï¼Œåå‘ï¼Œæ›´æ–°
- æµ‹è¯•æ¨¡å‹





### å·ç§¯ç¥ç»ç½‘ç»œ

***

